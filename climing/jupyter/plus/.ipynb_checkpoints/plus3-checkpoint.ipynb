{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e761e3d-b7f7-4397-8bb0-2b2ea09f3eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# import urllib.request\n",
    "# import urllib.parse\n",
    "# import json\n",
    "# from bs4 import BeautifulSoup\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.by import By\n",
    "# import time\n",
    "# import re\n",
    "# import pandas as pd\n",
    "\n",
    "# client_id = \"zCef739dSJ7Vgc0A6jXb\"\n",
    "# client_secret = \"HDOxDMIZ2m\"\n",
    "# search_keyword = \"수락산\"\n",
    "# mountain_road = urllib.parse.quote(search_keyword)\n",
    "# display = 100\n",
    "\n",
    "# # 버전에 상관 없이 os에 설치된 크롬 브라우저 사용\n",
    "# driver = webdriver.Chrome()\n",
    "# driver.implicitly_wait(3)\n",
    "\n",
    "# contents = []\n",
    "\n",
    "# for start in range(1, 902, 100):\n",
    "#     url = f'https://openapi.naver.com/v1/search/blog?query={mountain_road}&display={display}&start={start}'\n",
    "#     request = urllib.request.Request(url)\n",
    "#     request.add_header(\"X-Naver-Client-Id\", client_id)\n",
    "#     request.add_header(\"X-Naver-Client-Secret\", client_secret)\n",
    "#     response = urllib.request.urlopen(request)\n",
    "#     rescode = response.getcode()\n",
    "#     if rescode == 200:\n",
    "#         response_body = response.read()\n",
    "#         json_data = response_body.decode('utf-8')\n",
    "#         data_dict = json.loads(json_data)\n",
    "\n",
    "#         # \"items\" 키에 해당하는 값에서 \"link\" 키에 해당하는 값만 추출하여 리스트에 저장\n",
    "#         links = [item['link'] for item in data_dict['items']]\n",
    "#         print(start)\n",
    "\n",
    "#         # 각 블로그의 텍스트 가져오기\n",
    "#         for i in links:\n",
    "#             try:\n",
    "#                 print(i)\n",
    "#                 driver.get(i)\n",
    "#                 time.sleep(2)  # 대기시간 변경 가능\n",
    "\n",
    "#                 iframe = driver.find_element(By.ID , \"mainFrame\") # id가 mainFrame이라는 요소를 찾아내고 -> iframe임\n",
    "#                 driver.switch_to.frame(iframe) # 이 iframe이 내가 찾고자하는 html을 포함하고 있는 내용\n",
    "\n",
    "#                 source = driver.page_source\n",
    "#                 html = BeautifulSoup(source, \"html.parser\")\n",
    "                \n",
    "#                 # 기사 텍스트만 가져오기\n",
    "#                 content = html.select(\"div.se-main-container\")\n",
    "#                 #  list합치기\n",
    "#                 content = ''.join(str(content))\n",
    "\n",
    "#                 # html태그제거 및 텍스트 다듬기\n",
    "#                 pattern1 = '<[^>]*>'\n",
    "#                 content = re.sub(pattern=pattern1, repl='', string=content)\n",
    "#                 pattern2 = \"\"\"[\\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\"\"\"\n",
    "#                 content = content.replace(pattern2, '')\n",
    "#                 content = content.replace('\\n', '')\n",
    "#                 content = content.replace('\\u200b', '')\n",
    "#                 contents.append(content)\n",
    "#             except:\n",
    "#                 continue\n",
    "#     else:\n",
    "#         print(\"Error Code:\", rescode)\n",
    "\n",
    "# news_df = pd.DataFrame({'content': contents})\n",
    "# news_df.to_csv(f'{search_keyword}.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# # 크롬 드라이버 종료\n",
    "# driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fb49a2-23d3-4b71-b13e-4a0fe4aeecbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "https://duga.tistory.com/4079\n",
      "https://blog.naver.com/proletarian/223403356857\n",
      "https://blog.naver.com/goforit_k/222724556517\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "client_id = \"zCef739dSJ7Vgc0A6jXb\"\n",
    "client_secret = \"HDOxDMIZ2m\"\n",
    "search_keyword = \"승학산\"\n",
    "mountain_road = urllib.parse.quote(search_keyword)\n",
    "display = 100\n",
    "\n",
    "# 버전에 상관 없이 os에 설치된 크롬 브라우저 사용\n",
    "driver = webdriver.Chrome()\n",
    "driver.implicitly_wait(3)\n",
    "\n",
    "contents = []\n",
    "\n",
    "for start in range(1, 902, 100):\n",
    "    url = f'https://openapi.naver.com/v1/search/blog?query={mountain_road}&display={display}&start={start}'\n",
    "    request = urllib.request.Request(url)\n",
    "    request.add_header(\"X-Naver-Client-Id\", client_id)\n",
    "    request.add_header(\"X-Naver-Client-Secret\", client_secret)\n",
    "    response = urllib.request.urlopen(request)\n",
    "    rescode = response.getcode()\n",
    "    if rescode == 200:\n",
    "        response_body = response.read()\n",
    "        json_data = response_body.decode('utf-8')\n",
    "        data_dict = json.loads(json_data)\n",
    "\n",
    "        # \"items\" 키에 해당하는 값에서 \"link\" 키에 해당하는 값만 추출하여 리스트에 저장\n",
    "        links = [item['link'] for item in data_dict['items']]\n",
    "        print(start)\n",
    "\n",
    "        # 각 블로그의 텍스트 가져오기\n",
    "        for i in links:\n",
    "            try:\n",
    "                print(i)\n",
    "                driver.get(i)\n",
    "                time.sleep(2)  # 대기시간 변경 가능\n",
    "\n",
    "                iframe = driver.find_element(By.ID , \"mainFrame\") # id가 mainFrame이라는 요소를 찾아내고 -> iframe임\n",
    "                driver.switch_to.frame(iframe) # 이 iframe이 내가 찾고자하는 html을 포함하고 있는 내용\n",
    "\n",
    "                source = driver.page_source\n",
    "                html = BeautifulSoup(source, \"html.parser\")\n",
    "                \n",
    "                # 기사 텍스트만 가져오기\n",
    "                content = html.select(\"div.se-main-container\")\n",
    "                #  list합치기\n",
    "                content = ''.join(str(content))\n",
    "\n",
    "                # html태그제거 및 텍스트 다듬기\n",
    "                pattern1 = '<[^>]*>'\n",
    "                content = re.sub(pattern=pattern1, repl='', string=content)\n",
    "                pattern2 = \"\"\"[\\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\"\"\"\n",
    "                content = content.replace(pattern2, '')\n",
    "                content = content.replace('\\n', '')\n",
    "                content = content.replace('\\u200b', '')\n",
    "                contents.append(content)\n",
    "            except:\n",
    "                continue\n",
    "    else:\n",
    "        print(\"Error Code:\", rescode)\n",
    "\n",
    "news_df = pd.DataFrame({'content': contents})\n",
    "news_df.to_csv(f'{search_keyword}.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# 크롬 드라이버 종료\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb68f184-ed9c-4681-90d1-dfa757832dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "client_id = \"zCef739dSJ7Vgc0A6jXb\"\n",
    "client_secret = \"HDOxDMIZ2m\"\n",
    "search_keyword = \"식장산\"\n",
    "mountain_road = urllib.parse.quote(search_keyword)\n",
    "display = 100\n",
    "\n",
    "# 버전에 상관 없이 os에 설치된 크롬 브라우저 사용\n",
    "driver = webdriver.Chrome()\n",
    "driver.implicitly_wait(3)\n",
    "\n",
    "contents = []\n",
    "\n",
    "for start in range(1, 902, 100):\n",
    "    url = f'https://openapi.naver.com/v1/search/blog?query={mountain_road}&display={display}&start={start}'\n",
    "    request = urllib.request.Request(url)\n",
    "    request.add_header(\"X-Naver-Client-Id\", client_id)\n",
    "    request.add_header(\"X-Naver-Client-Secret\", client_secret)\n",
    "    response = urllib.request.urlopen(request)\n",
    "    rescode = response.getcode()\n",
    "    if rescode == 200:\n",
    "        response_body = response.read()\n",
    "        json_data = response_body.decode('utf-8')\n",
    "        data_dict = json.loads(json_data)\n",
    "\n",
    "        # \"items\" 키에 해당하는 값에서 \"link\" 키에 해당하는 값만 추출하여 리스트에 저장\n",
    "        links = [item['link'] for item in data_dict['items']]\n",
    "        print(start)\n",
    "\n",
    "        # 각 블로그의 텍스트 가져오기\n",
    "        for i in links:\n",
    "            try:\n",
    "                print(i)\n",
    "                driver.get(i)\n",
    "                time.sleep(2)  # 대기시간 변경 가능\n",
    "\n",
    "                iframe = driver.find_element(By.ID , \"mainFrame\") # id가 mainFrame이라는 요소를 찾아내고 -> iframe임\n",
    "                driver.switch_to.frame(iframe) # 이 iframe이 내가 찾고자하는 html을 포함하고 있는 내용\n",
    "\n",
    "                source = driver.page_source\n",
    "                html = BeautifulSoup(source, \"html.parser\")\n",
    "                \n",
    "                # 기사 텍스트만 가져오기\n",
    "                content = html.select(\"div.se-main-container\")\n",
    "                #  list합치기\n",
    "                content = ''.join(str(content))\n",
    "\n",
    "                # html태그제거 및 텍스트 다듬기\n",
    "                pattern1 = '<[^>]*>'\n",
    "                content = re.sub(pattern=pattern1, repl='', string=content)\n",
    "                pattern2 = \"\"\"[\\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\"\"\"\n",
    "                content = content.replace(pattern2, '')\n",
    "                content = content.replace('\\n', '')\n",
    "                content = content.replace('\\u200b', '')\n",
    "                contents.append(content)\n",
    "            except:\n",
    "                continue\n",
    "    else:\n",
    "        print(\"Error Code:\", rescode)\n",
    "\n",
    "news_df = pd.DataFrame({'content': contents})\n",
    "news_df.to_csv(f'{search_keyword}.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# 크롬 드라이버 종료\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8155980b-79e9-4c18-abfc-0724a81dd6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "client_id = \"zCef739dSJ7Vgc0A6jXb\"\n",
    "client_secret = \"HDOxDMIZ2m\"\n",
    "search_keyword = \"양성산\"\n",
    "mountain_road = urllib.parse.quote(search_keyword)\n",
    "display = 100\n",
    "\n",
    "# 버전에 상관 없이 os에 설치된 크롬 브라우저 사용\n",
    "driver = webdriver.Chrome()\n",
    "driver.implicitly_wait(3)\n",
    "\n",
    "contents = []\n",
    "\n",
    "for start in range(1, 902, 100):\n",
    "    url = f'https://openapi.naver.com/v1/search/blog?query={mountain_road}&display={display}&start={start}'\n",
    "    request = urllib.request.Request(url)\n",
    "    request.add_header(\"X-Naver-Client-Id\", client_id)\n",
    "    request.add_header(\"X-Naver-Client-Secret\", client_secret)\n",
    "    response = urllib.request.urlopen(request)\n",
    "    rescode = response.getcode()\n",
    "    if rescode == 200:\n",
    "        response_body = response.read()\n",
    "        json_data = response_body.decode('utf-8')\n",
    "        data_dict = json.loads(json_data)\n",
    "\n",
    "        # \"items\" 키에 해당하는 값에서 \"link\" 키에 해당하는 값만 추출하여 리스트에 저장\n",
    "        links = [item['link'] for item in data_dict['items']]\n",
    "        print(start)\n",
    "\n",
    "        # 각 블로그의 텍스트 가져오기\n",
    "        for i in links:\n",
    "            try:\n",
    "                print(i)\n",
    "                driver.get(i)\n",
    "                time.sleep(2)  # 대기시간 변경 가능\n",
    "\n",
    "                iframe = driver.find_element(By.ID , \"mainFrame\") # id가 mainFrame이라는 요소를 찾아내고 -> iframe임\n",
    "                driver.switch_to.frame(iframe) # 이 iframe이 내가 찾고자하는 html을 포함하고 있는 내용\n",
    "\n",
    "                source = driver.page_source\n",
    "                html = BeautifulSoup(source, \"html.parser\")\n",
    "                \n",
    "                # 기사 텍스트만 가져오기\n",
    "                content = html.select(\"div.se-main-container\")\n",
    "                #  list합치기\n",
    "                content = ''.join(str(content))\n",
    "\n",
    "                # html태그제거 및 텍스트 다듬기\n",
    "                pattern1 = '<[^>]*>'\n",
    "                content = re.sub(pattern=pattern1, repl='', string=content)\n",
    "                pattern2 = \"\"\"[\\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\"\"\"\n",
    "                content = content.replace(pattern2, '')\n",
    "                content = content.replace('\\n', '')\n",
    "                content = content.replace('\\u200b', '')\n",
    "                contents.append(content)\n",
    "            except:\n",
    "                continue\n",
    "    else:\n",
    "        print(\"Error Code:\", rescode)\n",
    "\n",
    "news_df = pd.DataFrame({'content': contents})\n",
    "news_df.to_csv(f'{search_keyword}.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# 크롬 드라이버 종료\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f672f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "client_id = \"zCef739dSJ7Vgc0A6jXb\"\n",
    "client_secret = \"HDOxDMIZ2m\"\n",
    "search_keyword = \"어등산\"\n",
    "mountain_road = urllib.parse.quote(search_keyword)\n",
    "display = 100\n",
    "\n",
    "# 버전에 상관 없이 os에 설치된 크롬 브라우저 사용\n",
    "driver = webdriver.Chrome()\n",
    "driver.implicitly_wait(3)\n",
    "\n",
    "contents = []\n",
    "\n",
    "for start in range(1, 902, 100):\n",
    "    url = f'https://openapi.naver.com/v1/search/blog?query={mountain_road}&display={display}&start={start}'\n",
    "    request = urllib.request.Request(url)\n",
    "    request.add_header(\"X-Naver-Client-Id\", client_id)\n",
    "    request.add_header(\"X-Naver-Client-Secret\", client_secret)\n",
    "    response = urllib.request.urlopen(request)\n",
    "    rescode = response.getcode()\n",
    "    if rescode == 200:\n",
    "        response_body = response.read()\n",
    "        json_data = response_body.decode('utf-8')\n",
    "        data_dict = json.loads(json_data)\n",
    "\n",
    "        # \"items\" 키에 해당하는 값에서 \"link\" 키에 해당하는 값만 추출하여 리스트에 저장\n",
    "        links = [item['link'] for item in data_dict['items']]\n",
    "        print(start)\n",
    "\n",
    "        # 각 블로그의 텍스트 가져오기\n",
    "        for i in links:\n",
    "            try:\n",
    "                print(i)\n",
    "                driver.get(i)\n",
    "                time.sleep(2)  # 대기시간 변경 가능\n",
    "\n",
    "                iframe = driver.find_element(By.ID , \"mainFrame\") # id가 mainFrame이라는 요소를 찾아내고 -> iframe임\n",
    "                driver.switch_to.frame(iframe) # 이 iframe이 내가 찾고자하는 html을 포함하고 있는 내용\n",
    "\n",
    "                source = driver.page_source\n",
    "                html = BeautifulSoup(source, \"html.parser\")\n",
    "                \n",
    "                # 기사 텍스트만 가져오기\n",
    "                content = html.select(\"div.se-main-container\")\n",
    "                #  list합치기\n",
    "                content = ''.join(str(content))\n",
    "\n",
    "                # html태그제거 및 텍스트 다듬기\n",
    "                pattern1 = '<[^>]*>'\n",
    "                content = re.sub(pattern=pattern1, repl='', string=content)\n",
    "                pattern2 = \"\"\"[\\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\"\"\"\n",
    "                content = content.replace(pattern2, '')\n",
    "                content = content.replace('\\n', '')\n",
    "                content = content.replace('\\u200b', '')\n",
    "                contents.append(content)\n",
    "            except:\n",
    "                continue\n",
    "    else:\n",
    "        print(\"Error Code:\", rescode)\n",
    "\n",
    "news_df = pd.DataFrame({'content': contents})\n",
    "news_df.to_csv(f'{search_keyword}.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# 크롬 드라이버 종료\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f2a966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "client_id = \"zCef739dSJ7Vgc0A6jXb\"\n",
    "client_secret = \"HDOxDMIZ2m\"\n",
    "search_keyword = \"연인산\"\n",
    "mountain_road = urllib.parse.quote(search_keyword)\n",
    "display = 100\n",
    "\n",
    "# 버전에 상관 없이 os에 설치된 크롬 브라우저 사용\n",
    "driver = webdriver.Chrome()\n",
    "driver.implicitly_wait(3)\n",
    "\n",
    "contents = []\n",
    "\n",
    "for start in range(1, 902, 100):\n",
    "    url = f'https://openapi.naver.com/v1/search/blog?query={mountain_road}&display={display}&start={start}'\n",
    "    request = urllib.request.Request(url)\n",
    "    request.add_header(\"X-Naver-Client-Id\", client_id)\n",
    "    request.add_header(\"X-Naver-Client-Secret\", client_secret)\n",
    "    response = urllib.request.urlopen(request)\n",
    "    rescode = response.getcode()\n",
    "    if rescode == 200:\n",
    "        response_body = response.read()\n",
    "        json_data = response_body.decode('utf-8')\n",
    "        data_dict = json.loads(json_data)\n",
    "\n",
    "        # \"items\" 키에 해당하는 값에서 \"link\" 키에 해당하는 값만 추출하여 리스트에 저장\n",
    "        links = [item['link'] for item in data_dict['items']]\n",
    "        print(start)\n",
    "\n",
    "        # 각 블로그의 텍스트 가져오기\n",
    "        for i in links:\n",
    "            try:\n",
    "                print(i)\n",
    "                driver.get(i)\n",
    "                time.sleep(2)  # 대기시간 변경 가능\n",
    "\n",
    "                iframe = driver.find_element(By.ID , \"mainFrame\") # id가 mainFrame이라는 요소를 찾아내고 -> iframe임\n",
    "                driver.switch_to.frame(iframe) # 이 iframe이 내가 찾고자하는 html을 포함하고 있는 내용\n",
    "\n",
    "                source = driver.page_source\n",
    "                html = BeautifulSoup(source, \"html.parser\")\n",
    "                \n",
    "                # 기사 텍스트만 가져오기\n",
    "                content = html.select(\"div.se-main-container\")\n",
    "                #  list합치기\n",
    "                content = ''.join(str(content))\n",
    "\n",
    "                # html태그제거 및 텍스트 다듬기\n",
    "                pattern1 = '<[^>]*>'\n",
    "                content = re.sub(pattern=pattern1, repl='', string=content)\n",
    "                pattern2 = \"\"\"[\\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\"\"\"\n",
    "                content = content.replace(pattern2, '')\n",
    "                content = content.replace('\\n', '')\n",
    "                content = content.replace('\\u200b', '')\n",
    "                contents.append(content)\n",
    "            except:\n",
    "                continue\n",
    "    else:\n",
    "        print(\"Error Code:\", rescode)\n",
    "\n",
    "news_df = pd.DataFrame({'content': contents})\n",
    "news_df.to_csv(f'{search_keyword}.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# 크롬 드라이버 종료\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
